########
#
# Template for deploying the Cluster Operator.
#
# Parameters:
#   CLUSTER_OPERATOR_NAMESPACE: namespace to which to deploy the cluster operator. Defaults to "openshift-cluster-operator".
#   SERVING_CA: base-64-encoded, pem CA cert for the ssl certs. Required.
#
########

apiVersion: v1
kind: Template
metadata:
  name: cluster-operator-deploy-template
  namespace: ${CLUSTER_OPERATOR_NAMESPACE}
objects:

# Service account for the API Server
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    name: cluster-operator-apiserver
    namespace: ${CLUSTER_OPERATOR_NAMESPACE}

# Service account for the Controller Manager
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    name: cluster-operator-controller-manager
    namespace: ${CLUSTER_OPERATOR_NAMESPACE}

# API Server gets the auth-delegator role to delegate auth decisions to
# the core API Server
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    name: "clusteroperator.openshift.io:apiserver-auth-delegator"
    namespace: ${CLUSTER_OPERATOR_NAMESPACE}
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:auth-delegator
  subjects:
  - apiGroup: ""
    kind: ServiceAccount
    name: cluster-operator-apiserver
    namespace: ${CLUSTER_OPERATOR_NAMESPACE}

# API Server gets the ability to read authentication. This allows it to
# read the specific configmap that has the requestheader-* entries to
# enable api aggregation
- apiVersion: rbac.authorization.k8s.io/v1
  kind: RoleBinding
  metadata:
    name: "clusteroperator.openshift.io:apiserver-authentication-reader"
    namespace: ${KUBE_SYSTEM_NAMESPACE}
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: Role
    name: extension-apiserver-authentication-reader
  subjects:
  - apiGroup: ""
    kind: ServiceAccount
    name: cluster-operator-apiserver
    namespace: ${CLUSTER_OPERATOR_NAMESPACE}

# Create a role for the Controller Manager to give it permissions to manage
# the resources of the clusteroperator
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    name: "clusteroperator.openshift.io:controller-manager"
  rules:
  # configmaps for leader election
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs:     ["create"]
  - apiGroups:     [""]
    resources:     ["configmaps"]
    resourceNames: ["cluster-operator-controller-manager"]
    verbs: ["get","update"]
  # events for recording events
  - apiGroups: [""]
    resources: ["events"]
    verbs:     ["create","patch","update"]
  # allow all operations on all resources in our API group
  - apiGroups: ["clusteroperator.openshift.io"]
    resources: ["*"]
    verbs:     ["create", "get", "list", "watch", "update", "patch", "delete"]
  # allow operations on required resources in any namespace a cluster is created
  - apiGroups:     [""]
    resources:     ["configmaps", "pods", "secrets", "serviceaccounts"]
    verbs: ["*"]
  - apiGroups:     ["batch"]
    resources:     ["jobs"]
    verbs: ["*"]
  # allow reading and creating rolebindings
  - apiGroups: ["rbac.authorization.k8s.io"]
    resources: ["rolebindings"]
    verbs: ["*"]

# Bind the Controller Manager service account to the role created for it
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    name: "clusteroperator.openshift.io:controller-manager"
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: "clusteroperator.openshift.io:controller-manager"
  subjects:
  - apiGroup: ""
    kind: ServiceAccount
    name: cluster-operator-controller-manager
    namespace: ${CLUSTER_OPERATOR_NAMESPACE}

- kind: PersistentVolumeClaim
  apiVersion: v1
  metadata:
    name: etcd-storage
  spec:
    accessModes:
      - ReadWriteOnce
    volumeMode: Filesystem
    resources:
      requests:
        storage: 1Gi

# Secret for AWS SSH private key
- apiVersion: v1
  kind: Secret
  metadata:
    name: ssh-private-key
    namespace: ${CLUSTER_OPERATOR_NAMESPACE}
    labels:
      app: cluster-operator
  type: Opaque
  data:
    ssh-privatekey: ${AWS_SSH_PRIVATE_KEY}

# Deployment of the API Server pod
- apiVersion: apps/v1beta1
  kind: Deployment
  metadata:
    name: cluster-operator-apiserver
    namespace: ${CLUSTER_OPERATOR_NAMESPACE}
    labels:
      app: cluster-operator-apiserver
  spec:
    selector:
      matchLabels:
        app: cluster-operator-apiserver
    template:
      metadata:
        labels:
          app: cluster-operator-apiserver
      spec:
        serviceAccountName: cluster-operator-apiserver
        containers:
        - name: apiserver
          args:
          - apiserver
          - --secure-port
          - "6443"
          - --etcd-servers
          - http://localhost:2379
          - -v
          - "10"
          image: cluster-operator:canary
          command: ["/opt/services/cluster-operator"]
          imagePullPolicy: Never
          name: apiserver
          ports:
          - containerPort: 6443
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          volumeMounts:
          - mountPath: /var/run/openshift-cluster-operator
            name: apiserver-ssl
            readOnly: true
          readinessProbe:
            httpGet:
              port: 6443
              path: /healthz
              scheme: HTTPS
            failureThreshold: 1
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          livenessProbe:
            httpGet:
              port: 6443
              path: /healthz
              scheme: HTTPS
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
        - name: etcd
          image: quay.io/coreos/etcd:latest
          imagePullPolicy: Always
          resources:
            requests:
              cpu: 100m
              memory: 20Mi
            limits:
              cpu: 100m
              memory: 30Mi
          env:
          - name: ETCD_DATA_DIR
            value: /etcd-data-dir
          command:
          - /usr/local/bin/etcd
          - --listen-client-urls
          - http://0.0.0.0:2379
          - --advertise-client-urls
          - http://localhost:2379
          ports:
          - containerPort: 2379
          volumeMounts:
          - name: etcd-data-dir
            mountPath: /etcd-data-dir
          readinessProbe:
            httpGet:
              port: 2379
              path: /health
            failureThreshold: 1
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          livenessProbe:
            httpGet:
              port: 2379
              path: /health
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: apiserver-ssl
          secret:
            defaultMode: 420
            secretName: cluster-operator-apiserver-cert
            items:
            - key: tls.crt
              path: apiserver.crt
            - key: tls.key
              path: apiserver.key
        - name: etcd-data-dir
          persistentVolumeClaim:
            claimName: etcd-storage

# Deployment of the Controller Manager pod
- kind: Deployment
  apiVersion: extensions/v1beta1
  metadata:
    name: cluster-operator-controller-manager
    namespace: ${CLUSTER_OPERATOR_NAMESPACE}
    labels:
      app: cluster-operator-controller-manager
  spec:
    selector:
      matchLabels:
        app: cluster-operator-controller-manager
    template:
      metadata:
        labels:
          app: cluster-operator-controller-manager
      spec:
        serviceAccountName: cluster-operator-controller-manager
        containers:
        - name: controller-manager
          image: cluster-operator:canary
          imagePullPolicy: Never
          resources:
            requests:
              cpu: 100m
              memory: 20Mi
            limits:
              cpu: 100m
              memory: 30Mi
          args:
          - controller-manager
          - --port
          - "8080"
          - --leader-election-namespace
          - ${CLUSTER_OPERATOR_NAMESPACE}
          - --leader-elect-resource-lock
          - "configmaps"
          - --profiling
          - "false"
          - --contention-profiling
          - "false"
          - -v
          - "1"
          - --log-level
          - "debug"
          ports:
          - containerPort: 8080
          readinessProbe:
            httpGet:
              port: 8080
              path: /healthz
            failureThreshold: 1
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2
          livenessProbe:
            httpGet:
              port: 8080
              path: /healthz
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 2

# Service for the API Server
- kind: Service
  apiVersion: v1
  metadata:
    name: cluster-operator-apiserver
    namespace: ${CLUSTER_OPERATOR_NAMESPACE}
    labels:
      app: cluster-operator-apiserver
  spec:
    type: NodePort
    selector:
      app: cluster-operator-apiserver
    ports:
    - name: secure
      protocol: TCP
      port: 443
      targetPort: 6443
      nodePort: 30443

# Registration of the cluster-operator API with the aggregator
- apiVersion: apiregistration.k8s.io/v1beta1
  kind: APIService
  metadata:
    name: v1alpha1.clusteroperator.openshift.io
    namespace: ${KUBE_SYSTEM_NAMESPACE}
  spec:
    group: clusteroperator.openshift.io
    version: v1alpha1
    service:
      namespace: ${CLUSTER_OPERATOR_NAMESPACE}
      name: cluster-operator-apiserver
    caBundle: ${SERVING_CA}
    groupPriorityMinimum: 10000
    versionPriority: 20

# Create a role for the master installer to save a kubeconfig
# from a managed cluster
- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    name: "clusteroperator.openshift.io:master-controller"
  rules:
  # CRUD secrets with kubeconfig data
  - apiGroups: [""]
    resources: ["secrets"]
    verbs:     ["create", "delete", "get", "list", "update"]

parameters:
# Namespace to use for cluster-operator
- name: CLUSTER_OPERATOR_NAMESPACE
  value: openshift-cluster-operator
# Namespace of kube-system. Do not change.
- name: KUBE_SYSTEM_NAMESPACE
  value: kube-system
# CA cert for API Server SSL cert
- name: SERVING_CA
- name: AWS_SSH_PRIVATE_KEY
